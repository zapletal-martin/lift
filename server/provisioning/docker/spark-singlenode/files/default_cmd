#!/bin/bash

LOCAL_IP=localhost
LOCAL_PORT=7077

echo "SPARK_SINGLE_NODE_IP=$LOCAL_IP"

echo "preparing Spark"
# This is now unfortunately required, because shell needs to be executed in special scope to be able to update env vars
sed -i s/__SPARK_LOCAL_IP__/$LOCAL_IP/ /opt/spark-$SPARK_VERSION/conf/spark-env.sh
sed -i s/__MASTER__/$LOCAL_IP/ /opt/spark-$SPARK_VERSION/conf/spark-env.sh
. /opt/spark-1.3.0/conf/spark-env.sh

env

echo "starting Hadoop Namenode"
hadoop namenode -format > /dev/null 2>&1
service hadoop-namenode start > /dev/null 2>&1
service hadoop-datanode start

echo "starting sshd"
/usr/sbin/sshd

sleep 5

echo "starting Spark Master at spark://$LOCAL_IP:$LOCAL_PORT"
nohup ${SPARK_HOME}/sbin/start-master.sh &

echo "starting Spark Worker at $LOCAL_IP"
nohup ${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker spark://$LOCAL_IP:$LOCAL_PORT -h $LOCAL_IP < /dev/null 2>&1 > /dev/null &
echo "worker started"

echo "starting driver $2 with classpath $1"
nohup java -jar $1 &

while [ 1 ];
do
	tail -f /opt/spark-${SPARK_VERSION}/logs/*.out
        sleep 1
done